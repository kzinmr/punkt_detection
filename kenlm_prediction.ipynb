{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kneser-Ney smoothingによるn-gram言語モデルを使った句点挿入(生文デモ)\n",
    "##  [KenLM](https://github.com/kpu/kenlm) で学習済みのモデル(*.klm)を読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram model\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "LM = 'data/model.100K.klm'\n",
    "model = kenlm.Model(LM)\n",
    "print('{0}-gram model'.format(model.order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力を分かち書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "\n",
    "class WordDivider:\n",
    "\n",
    "    def __init__(self, dictionary=\".mecabrc\", is_normalize=False):\n",
    "        self.dictionary = dictionary\n",
    "        self.tagger = MeCab.Tagger(self.dictionary)\n",
    "        self.tagger.parse(\"\") # necessary in Python3\n",
    "        self.is_normalize = is_normalize\n",
    "\n",
    "    def extract_words(self, text, with_pos=True):\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        words = []\n",
    "        # normalize text before MeCab processing\n",
    "        if self.is_normalize:\n",
    "            text = text.lower()\n",
    "\n",
    "        node = self.tagger.parseToNode(text)\n",
    "        while node:\n",
    "            features = node.feature.split(',')\n",
    "            # extract word surface (configurable)\n",
    "            word_to_append = node.surface\n",
    "            if with_pos:\n",
    "                pos = features[0]\n",
    "                word_to_append = '{}/{}'.format(word_to_append, pos)\n",
    "            words.append(word_to_append)\n",
    "            node = node.next\n",
    "\n",
    "        return words\n",
    "\n",
    "def wakati(line):\n",
    "    wd = WordDivider(is_normalize=True)\n",
    "    words = wd.extract_words(line, with_pos=False)\n",
    "    return ' '.join(filter(None, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句読点挿入モデル\n",
    "## 各位置で句点を挿入する/しないで文全体の尤度比較し、尤度が上がれば句点挿入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNKTS = ['、', '。']\n",
    "\n",
    "# windowはngramのnにあわせる\n",
    "def punkt_insert(sentence, punkt='。', window=3, debug=False):\n",
    "    words = sentence.split(' ')\n",
    "    pnkt_pos = []\n",
    "    for i in range(len(words)-window+1):\n",
    "        prob = model.score(' '.join(words))\n",
    "        prob_p = model.score(' '.join(words[:i+window] + [punkt] + words[i+window:]))\n",
    "        if prob_p > prob:\n",
    "            if debug:\n",
    "                print(prob_p, prob, words[i:i+window])\n",
    "            pnkt_pos.append(i+window)\n",
    "    return pnkt_pos\n",
    "\n",
    "def punkt_decode(sentence, punkt_pos, punkt='。'):\n",
    "    words = sentence.split(' ')\n",
    "    pnkt_sent = []\n",
    "    for i in range(len(words)):\n",
    "        pnkt_sent.append(words[i])\n",
    "        if i+1 in punkt_pos:\n",
    "            pnkt_sent.append(punkt)\n",
    "    return ' '.join(pnkt_sent)\n",
    "\n",
    "# 句読点付き文から句点挿入位置を抽出\n",
    "def extract_positions(sentence, punkt='。'):\n",
    "    true_positions = []\n",
    "    c_pos = 0\n",
    "    for w in sentence.split(' '):\n",
    "        if w in PUNKTS:\n",
    "            if w == punkt:\n",
    "                true_positions.append(c_pos)\n",
    "        else:\n",
    "            c_pos += 1\n",
    "    return true_positions\n",
    "\n",
    "# 句読点無し文に対する句点挿入位置の予測結果から精度と再現率を計算\n",
    "def prf(punkt_pred, punkt_true):\n",
    "    tp = set.intersection(set(punkt_pred), set(punkt_true))\n",
    "    recall = len(tp) / len(punkt_true) if len(punkt_true) > 0 else 0\n",
    "    precision = len(tp) / len(punkt_pred) if len(punkt_pred) > 0 else 0\n",
    "    fval = 2 / (1/precision + 1/recall) if precision and recall else 0.\n",
    "    return precision, recall, fval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句読点なし文に句読点を挿入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力文:\n",
      "今日 も 、 いい 天気 だ 。 明日 の 天気 も 晴れ らしい 。\n"
     ]
    }
   ],
   "source": [
    "sentence_true = '今日も、いい天気だ。明日の天気も晴れらしい。'\n",
    "sentence_true = wakati(sentence_true)\n",
    "print('入力文:')\n",
    "print(sentence_true)\n",
    "sentence_raw = ''.join(filter(lambda x: x not in PUNKTS, sentence_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句読点付き正解文と比較し、精度・再現率・F値を算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測結果:\n",
      "今日 も いい 天気 だ 。 明日 の 天気 も 晴れ らしい 。\n",
      "今日もいい天気だ。明日の天気も晴れらしい。\n",
      "[5, 11] [5, 11]\n",
      "Precision:1, Recall:1, F-value:1\n"
     ]
    }
   ],
   "source": [
    "sentence_wakati = wakati(sentence_raw)\n",
    "punkt_position = punkt_insert(sentence_wakati)\n",
    "sentence_pred = punkt_decode(sentence_wakati, punkt_position)\n",
    "print('予測結果:')\n",
    "print(sentence_pred)\n",
    "print(''.join(sentence_pred.split(' ')))\n",
    "print(extract_positions(sentence_pred), extract_positions(sentence_true))\n",
    "p,r,f = prf(extract_positions(sentence_pred), extract_positions(sentence_true))\n",
    "print('Precision:{0:.5g}, Recall:{1:.5g}, F-value:{2:.5g}'.format(p,r,f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
